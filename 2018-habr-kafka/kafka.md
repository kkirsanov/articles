# Опыт построения инфраструктуры на микросервисной архитектуре

В статье описывается опыт перевода крупной финтех python инфраструктуры состоящей из нескольких монолитных приложений связанных синхронными протоколами на микросервисную архитектуру

<cut />

## Введение

За последние пол гда публикаций о микросервисах стало так много, что рассказывать что это и зачем нужно было бы пустой тратой времени, так что дальшейншее изложение будет сконцентрировано на воросе - что мы сделали и как.

У нас в небольшом банке были большие проблемы: монструозное монолитное python приложение из 4-5 отдельных кусков (один даже на php) связанных чудовищным количесвтом синхронных RPC взаимодейсвий с большим объемом legacy.

Что бы хотя бы от части решить все возникающие при этом проблемы было принято решение перейти на микросервисную архитектуру. Но прежде чем регшиться на такой шаг нужно ответить на ряд вопросов:

- Как разбить монллит на микросервисы и какими криетриями следует при этом руководсоваться.
- Каким образом микросеврисы будут взаимодействовать?
- Каким будет протокол и как его обновлять?
- Что делать с ошибками?
- Как организовать мониторнг?


Собсвенно ответам на эти вопросы и будет посвящена данная статья. Следует отметить что для меня это был первый опыт проектирвоания подобной системы, так что при выоре технологий и алгоритомв во главу угла ставилась простота и предсказуемость результата.


##Каким образом разбить монллит на микросервисы и какими криетриями следует при этом руководсоваться.
Этот, казалось бы простой вопрос, определил в конечном итоге всю дальнейшую архитектру. У нас есть две крайности - не делать ничего и каждую строчку кода сделать отдельным сервисом. Где то между ними и лежит наше решение.

Мы - банк, соответвсенно вся система крутиться вокруг операций с финансами и различными вспомогательными вещами. Перенсти [финансовые ACID транзакци на распределенную систему с сагами](https://habr.com/company/avito/blog/426101/) безусловно можно, но в общем случае крайне трудно. Таким образом мы выробатали следущие криетрии: 
- Соблюдать S из SOLID применительно к микросервисам
- Транзакция должна целиком осущетсвляться в микросервисе.
- Для работы микросервису в основном нужна инофрмация из его собсвенной базы данных.

Увы, реализовать чистоту (в смысле функциольных языков) для микросервисов как хотелось сначала оказалось весьма непросто.

### Каким образом микросеврисы будут взаимодейсвовать?
Вариантов множестово, но в конечном итоге их всех можно абстрагирвоать простым "микросервисы обмениваются сообщениями", но если реализовать синхронный протокол (напрмер RPC через REST) то большнесво недостатков монолита сохранятся, а вот достоинств микросервисов почти не появится. Так что очевиднным решеним было взять любой брокер сообщений и начать работать. Выбирая между RabbitMQ и Кафкой остановились на последней и вот почему:

- Кафка проще и предоставляет единсвеннау модель передачи сообщений - [Publish–subscribe](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern)
- Можно сравнительно просто получить данные из кафки второй раз. Это чрезвычайно удобно для отладки или исправления багов при некорректной обработке а так же для мониторинга.
- Понятный и простой способ маштабирвоания: добавили партиций в топик, запустили больше воркеров. Остальное сделает кафка

Очереди на кафке+асинхронность позволяют нам:
- Ненадолго выключать любой микросервис для обновлений без заметных полсейдвий для остальных
- На долго выключать любой сервис и не возщиться с восстановлением данных. Напирмер неавдно падал микросервис фискалзиции. Починили через 2 часа, он запбрал необработаные счета из кафки и всё проёл. Не нужно было как раньше по HTTP логам восстанавливать что там должно было произойти и вручную проводить.
- Запусать тестовые ваиранты сервисов на актуальных данных с прода и сравнивать резульатт их обработки с версией сервиса на проде.

Но есть и проблемы, связаные c моделью publish-subsribe: В нашей системе при успешной фиксации платёжной транзакии отправляется сообщение с данными о преокте, партенре, суммах и статусе платежа. Эту информацию получает сервис статистики, что бы агрегирвоать и показать партнеру график в личном кабинете, та же система фикализации, что бы обратиться к оператору вискальных данных, сформирвоать чек и передать его партнеру а ещё1 есть малкеьний сепрвис, которым пользуется 2 партенра и доля транзакций которог ов обзем потоке меньше 0.0001%. И он вынужден получать и обрабатывать информцию обо всех платежах. Кстаться делает он это быстро - примерно 5 тыс транзакций в секунду, так что пока это не представляет проблемы. Если же это станет проблоемой то можно будет мделать отдельный сервис-роутер, который будет расклажывать сообщения по нескольким разным топикам, тем самым реализоват часть функционала RabbitMQ,  отсутвующего в кафке.

Ещё одна тонкость: в настрйоках кафки есть параметр ограничивающие время, которые кафка "помнит" на каком месте читатель остановился - по умолчанию 2 дня. Хорошо бы поднять до недели, что бы если проблема возникнит в праздиники и 2 дня не будет решена, то это не привело бы к потоере прозиции в топике.

## Каким будет протокол и как его обновлять?
В качесве системы сериализации данных мы выбрали AVRO, почему - описано в [отдельной статье](https://habr.com/post/346698/).

Но вне зависимости от выбраного способа сериализации важно понимать как будет проходить обновление проткола. Хотя AVRO и поддерживает [Schema Resolution](https://avro.apache.org/docs/1.8.2/spec.html#Schema+Resolution) мы этим не пользуемся и решаем чисто административно:

- Данные в топики пишутся и читаются только через AVRO, название топика соответсвует названию схемы
- Если нужно дополнить или изменить данные, то создается новая схема с новым топиком в кафке

Так же для обеспечения мониторинга и логирования мы применяем следущий подход: каждый сервис при обработке сообщения дополняет его метаинформацией, содержащей:

- название сервиса
- UUID данного процесса побработки сообщения
- timestamp начала процесса
- длительность процесса

В резульате по мере прохожедния сообщения через вычислительный граф сообщенгие обогащается информацией о пройденом на графе пути. Получается аналог zipkin/opentracing но для асинхрогогого взаимодейсвия.

Особую ценность это приобритает в тех случаях, когда на графе вознкают циклы. Помните пример с маленьким сервисом, доля в платежах которог осостяавляет всего 0.0001%? Анализируя мета-информацию в сообщении он может опредилить - являлся ли они инициатором платежа, не обращаясь при  этом в БД для сверки.



## Что делать с ошибками?
