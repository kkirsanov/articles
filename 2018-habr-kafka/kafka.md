# Опыт построения инфраструктуры на микросервисной архитектуре

**картинка с адским графом**
В статье описывается опыт перевода крупной финтех python инфраструктуры состоящей из нескольких монолитных приложений связанных синхронными протоколами на микросервисную архитектуру

<cut />

## Введение

За последние полгода публикаций о микросервисах стало так много, что рассказывать что это и зачем нужно было бы пустой тратой времени, так что дальнейшее изложение будет сконцентрировано на вопросе - что мы сделали и как.

У нас в небольшом банке были большие проблемы: монструозное монолитное python приложение из 4-5 отдельных кусков (один даже на php) связанных чудовищным количеством синхронных RPC взаимодействий с большим объемом legacy.

Что бы хотя бы отчасти решить все возникающие при этом проблемы было принято решение перейти на микросервисную архитектуру. Но прежде чем решиться на такой шаг нужно ответить на ряд вопросов:

- Как разбить монолит на микросервисы и какими критериями следует при этом руководствоваться.
- Каким образом микросервисы будут взаимодействовать?
- Каким будет протокол и как его обновлять?
- Что делать с ошибками?
- Как организовать мониторинг?


Собственно ответам на эти вопросы и будет посвящена данная статья. Следует отметить что для меня это был первый опыт проектирования подобной системы, так что при выборе технологий и алгоритмов во главу угла ставилась простота и предсказуемость результата.


## Каким образом разбить монолит на микросервисы и какими критериями следует при этом руководствоваться.

**картинка с акторами и эрлангом**
Этот, казалось бы простой вопрос, определил в конечном итоге всю дальнейшую архитектуру. У нас есть две крайности - не делать ничего и каждую строчку кода сделать отдельным сервисом. Где то между ними и лежит наше решение.

Мы - банк, соответственно вся система крутиться вокруг операций с финансами и различными вспомогательными вещами. Перенсти [финансовые ACID транзакци на распределенную систему с сагами](https://habr.com/company/avito/blog/426101/) безусловно можно, но в общем случае крайне трудно. Таким образом мы выработали следующие критерии:
- Соблюдать S из SOLID применительно к микросервисам
- Транзакция должна целиком осуществляться в микросервисе.
- Для работы микросервису в основном нужна инофрмация из его собственной базы данных.

Увы, реализовать чистоту (в смысле функциональных языков) для микросервисов как хотелось сначала оказалось весьма непросто.

### Каким образом микросеврисы будут взаимодейсвовать?
Вариантов множестово, но в конечном итоге их всех можно абстрагировать простым "микросервисы обмениваются сообщениями", но если реализовать синхронный протокол (например RPC через REST) то большинство недостатков монолита сохранятся, а вот достоинств микросервисов почти не появится. Так что очевидным решением было взять любой брокер сообщений и начать работать. Выбирая между RabbitMQ и Кафкой остановились на последней и вот почему:

- Кафка проще и предоставляет единственную модель передачи сообщений - [Publish–subscribe](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern)
- Можно сравнительно просто получить данные из кафки второй раз. Это чрезвычайно удобно для отладки или исправления багов при некорректной обработке а также для мониторинга.
- Понятный и простой способ масштабирования: добавили партиций в топик, запустили больше воркеров. Остальное сделает кафка. Дополнительно хочу обратить внимание на очень качественное и детальное сравнение - https://habr.com/company/itsumma/blog/416629/

Очереди на кафке+асинхронность позволяют нам:
- Ненадолго выключать любой микросервис для обновлений без заметных последствий для остальных
- Надолго выключать любой сервис и не возиться с восстановлением данных. Например недавно падал микросервис фискалзиции. Починили через 2 часа, он забрал необработанные счета из кафки и всё прошёл. Не нужно было как раньше по HTTP логам восстанавливать что там должно было произойти и вручную проводить.
- Запускать тестовые варианты сервисов на актуальных данных с прода и сравнивать результаты их обработки с версией сервиса на проде.

Но есть и проблемы, связанные c моделью publish-subsribe: В нашей системе при успешной фиксации платежной транзакции отправляется сообщение с данными о проекте, партнере, суммах и статусе платежа. Эту информацию получает сервис статистики, чтобы агрегировать и показать партнеру график в личном кабинете, та же система фискализации, чтобы обратиться к оператору фискальных данных, сформировать чек и передать его партнеру а ещё есть маленький сервис, которым пользуется 2 партнера и доля транзакций которого в общем потоке меньше 0.0001%. И он вынужден получать и обрабатывать информацию обо всех платежах. Кстати делает он это быстро - примерно 5 тыс транзакций в секунду, так что пока это не представляет проблемы. Если же это станет проблемой то можно будет сделать отдельный сервис-роутер, который будет раскладывать сообщения по нескольким разным топикам, тем самым реализовывать часть функционала RabbitMQ,  отсутствующего в кафке.

Ещё одна тонкость: в настройках кафки есть параметр ограничивающие время, которые кафка "помнит" на каком месте читатель остановился - по умолчанию 2 дня. Хорошо бы поднять до недели, чтобы если проблема возникает в праздники и 2 дня не будет решена, то это не привело бы к потере позиции в топике.

## Каким будет протокол и как его обновлять?
В качестве системы сериализации данных мы выбрали AVRO, почему - описано в [отдельной статье](https://habr.com/post/346698/).

Но вне зависимости от выбранного способа сериализации важно понимать как будет проходить обновление протокола. Хотя AVRO и поддерживает [Schema Resolution](https://avro.apache.org/docs/1.8.2/spec.html#Schema+Resolution) мы этим не пользуемся и решаем чисто административно:

- Данные в топики пишутся и читаются только через AVRO, название топика соответствует названию схемы
- Если нужно дополнить или изменить данные, то создается новая схема с новым топиком в кафке

Сами же схемы AVRO мы храним в git-субмодулях и подключаем ко всем кафка-проектам. Централизованный Реестр схем решили пока не внедрять.


***план обновления**
**проблема единсвенного адресата**

Так же для обеспечения мониторинга и логирования мы применяем следующий подход: каждый сервис при обработке сообщения дополняет его метаинформацией, содержащей:

- название сервиса
- UUID данного процесса обработки сообщения
- timestamp начала процесса
- длительность процесса

В результате по мере прохождения сообщения через вычислительный граф сообщение обогащается информацией о пройденном на графе пути. Получается аналог zipkin/opentracing но для асинхронного взаимодействия.

Особую ценность это приобретает в тех случаях, когда на графе возникают циклы. Помните пример с маленьким сервисом, доля в платежах которого составляет всего 0.0001%? Анализируя мета-информацию в сообщении он может опредилить - являлся ли они инициатором платежа, не обращаясь при  этом в БД для сверки.

## Что делать с ошибками?

**python - exception из воздуха. Лучше бы писали на Яве**

**классификация ошибок**

**повторы**

