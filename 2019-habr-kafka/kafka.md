# Опыт построения инфраструктуры на микросервисной архитектуре

В статье кратико описывается опыт перевода финтех python инфраструктуры состоящей из нескольких монолитных приложений связанных синхронными протоколами на микросервисную архитектуру

<cut />

## Введение

За последние полгода публикаций о микросервисах стало так много, что рассказывать что это и зачем нужно было бы пустой тратой времени, так что дальнейшее изложение будет сконцентрировано на вопросе - что мы сделали и как.

У нас в небольшом банке были большие проблемы: монструозное монолитное python приложение из 4-5 отдельных кусков (один даже на php) связанных чудовищным количеством синхронных RPC взаимодействий с большим объемом legacy. Что бы хотя бы отчасти решить все возникающие при этом проблемы было принято решение перейти на микросервисную архитектуру. Но прежде чем решиться на такой шаг нужно ответить на ряд вопросов:

- Как разбить монолит на микросервисы и какими критериями следует при этом руководствоваться.
- Каким образом микросервисы будут взаимодействовать?
- Как всё это мониторить?
- Каким будет протокол и как его обновлять?


Собственно кратиким ответам на эти вопросы и будет посвящена данная статья. 

## Каким образом разбить монолит на микросервисы и какими критериями следует при этом руководствоваться.

Этот, казалось бы простой вопрос, определил в конечном итоге всю дальнейшую архитектуру. У нас есть две крайности - не делать ничего и каждую строчку кода сделать отдельным сервисом. Где то между ними и лежит решение.

Мы - банк, соответственно вся система крутиться вокруг операций с финансами и различными вспомогательными вещами. Перенсти [финансовые ACID транзакци на распределенную систему с сагами](https://habr.com/company/avito/blog/426101/) безусловно можно, но в общем случае крайне трудно. Таким образом мы выработали следующие критерии:
- Соблюдать S из SOLID применительно к микросервисам
- Транзакция должна целиком осуществляться в микросервисе.
- Для работы микросервису в основном нужна инофрмация из его собственной базы данных.

Увы, реализовать чистоту (в смысле функциональных языков) для микросервисов как хотелось сначала оказалось весьма непросто.

## Каким образом микросеврисы будут взаимодейсвовать?
Вариантов множестово, но в конечном итоге их всех можно абстрагировать простым "микросервисы обмениваются сообщениями", но если реализовать синхронный протокол (например RPC через REST) то большинство недостатков монолита сохранятся, а вот достоинств микросервисов почти не появится. Так что очевидным решением было взять любой брокер сообщений и начать работать. Выбирая между RabbitMQ и Кафкой остановились на последней и вот почему:

- Кафка проще и предоставляет единственную модель передачи сообщений - [Publish–subscribe](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern)
- Можно сравнительно просто получить данные из кафки второй раз. Это чрезвычайно удобно для отладки или исправления багов при некорректной обработке а также для мониторинга.
- Понятный и простой способ масштабирования: добавили партиций в топик, запустили больше воркеров. Остальное сделает кафка. 

Дополнительно хочу обратить внимание на очень качественное и детальное сравнение - https://habr.com/company/itsumma/blog/416629/

Очереди на кафке+асинхронность позволяют нам:
- Ненадолго выключать любой микросервис для обновлений без заметных последствий для остальных
- Надолго выключать любой сервис и не возиться с восстановлением данных. Например недавно падал микросервис фискалзиции. Починили через 2 часа, он забрал необработанные счета из кафки и всё обработал. Не нужно было как раньше по HTTP логам иои по отдельной таблице в БД восстанавливать что там должно было произойти и вручную проводить.
- Запускать тестовые варианты сервисов на актуальных данных с прода и сравнивать результаты их обработки с версией сервиса на проде.

Но есть и проблемы, связанные c моделью publish-subsribe: В нашей системе при успешной фиксации платежной транзакции отправляется сообщение с данными о проекте, партнере, суммах и статусе платежа. Эту информацию получает сервис статистики, чтобы агрегировать и показать партнеру график в личном кабинете, та же система фискализации, чтобы обратиться к оператору фискальных данных, сформировать чек и передать его партнеру а ещё есть маленький сервис, которым пользуется 2 партнера и доля транзакций которого в общем потоке меньше 0.0001%. И он вынужден получать и обрабатывать информацию обо всех платежах. Кстати делает он это быстро - примерно 5 тыс транзакций в секунду, так что пока это не представляет проблемы. Если же это станет проблемой то можно будет сделать отдельный сервис-роутер, который будет раскладывать сообщения по нескольким разным топикам, тем самым реализовывать часть функционала RabbitMQ,  отсутствующего в кафке.

Ещё одна тонкость: в настройках кафки есть параметр ограничивающие время, которые кафка "помнит" на каком месте читатель остановился - по умолчанию 2 дня. Хорошо бы поднять до недели, чтобы если проблема возникает в праздники и 2 дня не будет решена, то это не привело бы к потере позиции в топике. А как эти взаимодейсвия рисовать? Мы остановились на graphviz. Если по-честному нарисовать все взаимосвязи то  традиционный для микросервисов ёжик апокалипсиса с десятками мутных звисимостей.

Что бы хоть как то сделать его (граф связей) читаемым мы договорились о слежующей нотации: микросервисы - овалы, топики кафки - прямоугольники.
Таким образом на одном графе удаётся отобразить и факт враимодейсвия и его тип.

## Как всё это мониторить?
Мониторинг внедряли постепенно по мере разратания парка кафочных сервисов.

Сначала мониторили читая локи консьбмеров. Нет логов - нет работы.
За тем исползуя CLI кафки. Но что бы опросить все топики уходило до 1-й минуты, так что в конце конов перешли на последний вариант: связка [kafka-exporter](https://github.com/danielqsj/kafka_exporter) -> [prometheus](https://prometheus.io/) -> [grafana](https://grafana.com/)

И чуть подправленный дашборд от kafka-exporter - https://github.com/kkirsanov/articles/blob/master/2018-habr-kafka/dashboard.json

В результате получаем вот такие картинки:
![](https://habrastorage.org/webt/lg/bg/k5/lgbgk5zs7fzl_3hrhork-ybejjw.png)

Мониторинг сразу высылает скриншот возникающей проблемы и ориентируюсь на имя кончьюмера который вдруг перестал читать разбираемся с проблемой.


Так же для обеспечения мониторинга и логирования мы применяем следующий подход: каждый сервис при обработке сообщения дополняет его метаинформацией содержащей trace_id и список записей типа:

- название сервиса
- UUID процесса обработки
- timestamp начала процесса
- длительность процесса
- набор тегов

В результате по мере прохождения сообщения через вычислительный граф сообщение обогащается информацией о пройденном на графе пути. Получается эдакий аналог zipkin/opentracing, позволяющий получив сообщение легко ответить на вопрос "сколько времени прошло от момента когда мы приняли входящую СМС, то дотого как партнёр получил нотификацию о нехватке средств"

Особую ценность это приобретает в тех случаях, когда на графе возникают циклы. Помните пример с маленьким сервисом, доля в платежах которого составляет всего 0.0001%? Анализируя мета-информацию в сообщении он может опредилить - являлся ли они инициатором платежа, не обращаясь при  этом в БД для сверки.

## Каким будет протокол и как его обновлять?
В качестве системы сериализации данных мы выбрали AVRO, почему - описано в [отдельной статье](https://habr.com/post/346698/).

Но вне зависимости от выбранного способа сериализации важно понимать как будет проходить обновление протокола. Хотя AVRO и поддерживает [Schema Resolution](https://avro.apache.org/docs/1.8.2/spec.html#Schema+Resolution) мы этим не пользуемся и решаем чисто административно:

- Данные в топики пишутся и читаются только через AVRO, название топика соответствует названию схемы
- Если нужно дополнить или изменить данные, то создается новая схема с новым топиком в кафке, после чего:
- Сначала все продьюсеры переключаеются на новый топик, а за ними - консьюмеры 

Сами же схемы AVRO мы храним в git-субмодулях и подключаем ко всем кафка-проектам. Централизованный Реестр схем решили пока внедрять.